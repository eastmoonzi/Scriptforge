"""
è¯„æµ‹ä½“ç³» - Evaluation System
ç”¨äºé‡åŒ–è¯„ä¼°å¤šè§’è‰²å¯¹è¯ç³»ç»Ÿçš„è´¨é‡

æ ¸å¿ƒæŒ‡æ ‡ï¼š
1. äººè®¾ç¦»æ•£åº¦ï¼ˆCharacter Personality Divergence, CPDï¼‰
2. å¯¹è¯æœ‰æ•ˆç‡ï¼ˆDialogue Efficiency, DEï¼‰
3. OOC ç‡ï¼ˆOut of Character Rateï¼‰
"""

import json
import re
from typing import List, Dict, Tuple, Optional
from datetime import datetime
from collections import Counter, defaultdict
import math


class EvaluationMetrics:
    """è¯„æµ‹æŒ‡æ ‡è®¡ç®—å™¨"""

    def __init__(self, api_key: Optional[str] = None):
        """
        åˆå§‹åŒ–è¯„æµ‹ç³»ç»Ÿ

        Args:
            api_key: Gemini API Keyï¼ˆç”¨äºè¯­ä¹‰åˆ†æï¼‰
        """
        self.api_key = api_key
        self.llm = None

        if api_key:
            try:
                import google.genai as genai
                self.llm = genai.Client(api_key=api_key)
            except ImportError:
                print("âš ï¸ google-genai æœªå®‰è£…ï¼Œå°†ä½¿ç”¨åŸºç¡€æŒ‡æ ‡")

    # ==================== 1. äººè®¾ç¦»æ•£åº¦ï¼ˆCPDï¼‰====================

    def calculate_cpd(self, conversations: List[Dict]) -> Dict:
        """
        è®¡ç®—äººè®¾ç¦»æ•£åº¦ï¼ˆCharacter Personality Divergenceï¼‰

        è¡¡é‡ä¸åŒè§’è‰²å‘è¨€çš„å·®å¼‚ç¨‹åº¦ï¼Œæ•°å€¼è¶Šé«˜è¡¨ç¤ºè§’è‰²æ€§æ ¼è¶Šé²œæ˜

        Args:
            conversations: å¯¹è¯åˆ—è¡¨ [{'speaker': 'è§’è‰²å', 'content': 'å†…å®¹'}, ...]

        Returns:
            {
                'cpd_score': float,  # 0-100ï¼Œè¶Šé«˜è¶Šå¥½
                'details': {
                    'vocabulary_diversity': float,  # è¯æ±‡å¤šæ ·æ€§
                    'length_variance': float,       # å‘è¨€é•¿åº¦å·®å¼‚
                    'punctuation_diversity': float, # æ ‡ç‚¹é£æ ¼å·®å¼‚
                    'character_scores': dict        # æ¯ä¸ªè§’è‰²çš„å¾—åˆ†
                }
            }
        """
        # æŒ‰è§’è‰²åˆ†ç»„å¯¹è¯
        character_messages = defaultdict(list)
        for msg in conversations:
            if msg.get('speaker') and msg.get('content'):
                character_messages[msg['speaker']].append(msg['content'])

        if len(character_messages) < 2:
            return {'cpd_score': 0, 'details': {}}

        # 1. è¯æ±‡å¤šæ ·æ€§åˆ†æ
        vocab_diversity = self._calculate_vocabulary_diversity(character_messages)

        # 2. å‘è¨€é•¿åº¦å·®å¼‚
        length_variance = self._calculate_length_variance(character_messages)

        # 3. æ ‡ç‚¹é£æ ¼å·®å¼‚
        punct_diversity = self._calculate_punctuation_diversity(character_messages)

        # ç»¼åˆå¾—åˆ†ï¼ˆåŠ æƒå¹³å‡ï¼‰
        cpd_score = (
            vocab_diversity * 0.5 +      # è¯æ±‡å·®å¼‚å 50%
            length_variance * 0.3 +       # é•¿åº¦å·®å¼‚å 30%
            punct_diversity * 0.2         # æ ‡ç‚¹å·®å¼‚å 20%
        )

        return {
            'cpd_score': round(cpd_score, 2),
            'details': {
                'vocabulary_diversity': round(vocab_diversity, 2),
                'length_variance': round(length_variance, 2),
                'punctuation_diversity': round(punct_diversity, 2),
                'character_count': len(character_messages)
            }
        }

    def _calculate_vocabulary_diversity(self, character_messages: Dict[str, List[str]]) -> float:
        """è®¡ç®—è§’è‰²é—´è¯æ±‡å¤šæ ·æ€§"""
        character_vocabs = {}

        for char, messages in character_messages.items():
            # æå–è¯æ±‡ï¼ˆç®€å•åˆ†è¯ï¼‰
            words = []
            for msg in messages:
                # ç§»é™¤æ ‡ç‚¹ï¼Œåˆ†è¯
                words.extend(re.findall(r'\w+', msg.lower()))

            # è¯é¢‘ç»Ÿè®¡
            character_vocabs[char] = Counter(words)

        # è®¡ç®—è§’è‰²é—´çš„è¯æ±‡é‡å åº¦ï¼ˆJaccard ç›¸ä¼¼åº¦çš„åå‘ï¼‰
        chars = list(character_vocabs.keys())
        if len(chars) < 2:
            return 0

        overlaps = []
        for i in range(len(chars)):
            for j in range(i + 1, len(chars)):
                vocab1 = set(character_vocabs[chars[i]].keys())
                vocab2 = set(character_vocabs[chars[j]].keys())

                if not vocab1 or not vocab2:
                    continue

                # Jaccard ç›¸ä¼¼åº¦
                intersection = len(vocab1 & vocab2)
                union = len(vocab1 | vocab2)
                similarity = intersection / union if union > 0 else 0

                # å·®å¼‚åº¦ = 1 - ç›¸ä¼¼åº¦
                overlaps.append(1 - similarity)

        # å¹³å‡å·®å¼‚åº¦ï¼Œè½¬æ¢ä¸º 0-100 åˆ†
        avg_diversity = sum(overlaps) / len(overlaps) if overlaps else 0
        return avg_diversity * 100

    def _calculate_length_variance(self, character_messages: Dict[str, List[str]]) -> float:
        """è®¡ç®—è§’è‰²å‘è¨€é•¿åº¦çš„æ–¹å·®"""
        char_avg_lengths = {}

        for char, messages in character_messages.items():
            lengths = [len(msg) for msg in messages]
            char_avg_lengths[char] = sum(lengths) / len(lengths) if lengths else 0

        if len(char_avg_lengths) < 2:
            return 0

        # è®¡ç®—æ–¹å·®
        avg_lengths = list(char_avg_lengths.values())
        mean = sum(avg_lengths) / len(avg_lengths)
        variance = sum((x - mean) ** 2 for x in avg_lengths) / len(avg_lengths)

        # å½’ä¸€åŒ–åˆ° 0-100ï¼ˆå‡è®¾æ ‡å‡†å·®åœ¨ 0-50 å­—ç¬¦èŒƒå›´ï¼‰
        std_dev = math.sqrt(variance)
        normalized = min(std_dev / 50 * 100, 100)

        return normalized

    def _calculate_punctuation_diversity(self, character_messages: Dict[str, List[str]]) -> float:
        """è®¡ç®—æ ‡ç‚¹ç¬¦å·ä½¿ç”¨é£æ ¼çš„å·®å¼‚"""
        char_punct_patterns = {}

        for char, messages in character_messages.items():
            punct_counts = Counter()
            for msg in messages:
                # ç»Ÿè®¡æ ‡ç‚¹ç¬¦å·
                puncts = re.findall(r'[ï¼ï¼Ÿã€‚ï¼Œã€ï¼›ï¼š""''â€¦â€”ï¼ˆï¼‰ã€Šã€‹ã€ã€‘]', msg)
                punct_counts.update(puncts)

            # æ ‡ç‚¹ç¬¦å·åˆ†å¸ƒï¼ˆå½’ä¸€åŒ–ï¼‰
            total = sum(punct_counts.values())
            if total > 0:
                char_punct_patterns[char] = {
                    p: count / total for p, count in punct_counts.items()
                }
            else:
                char_punct_patterns[char] = {}

        # è®¡ç®—è§’è‰²é—´æ ‡ç‚¹åˆ†å¸ƒçš„å·®å¼‚ï¼ˆä½¿ç”¨ KL æ•£åº¦çš„ç®€åŒ–ç‰ˆï¼‰
        chars = list(char_punct_patterns.keys())
        if len(chars) < 2:
            return 0

        differences = []
        for i in range(len(chars)):
            for j in range(i + 1, len(chars)):
                pattern1 = char_punct_patterns[chars[i]]
                pattern2 = char_punct_patterns[chars[j]]

                if not pattern1 or not pattern2:
                    continue

                # ç®€åŒ–çš„å·®å¼‚åº¦ï¼šç»Ÿè®¡ä¸åŒçš„æ ‡ç‚¹ç§ç±»å æ¯”
                all_puncts = set(pattern1.keys()) | set(pattern2.keys())
                diff = sum(abs(pattern1.get(p, 0) - pattern2.get(p, 0))
                          for p in all_puncts)
                differences.append(diff)

        avg_diff = sum(differences) / len(differences) if differences else 0
        return min(avg_diff * 100, 100)

    # ==================== 2. å¯¹è¯æœ‰æ•ˆç‡ï¼ˆDEï¼‰====================

    def calculate_de(self, conversations: List[Dict]) -> Dict:
        """
        è®¡ç®—å¯¹è¯æœ‰æ•ˆç‡ï¼ˆDialogue Efficiencyï¼‰

        è¡¡é‡å¯¹è¯ä¸­æœ‰æ•ˆä¿¡æ¯çš„å¯†åº¦ï¼Œæ•°å€¼è¶Šé«˜è¡¨ç¤ºå¯¹è¯è¶Šé«˜æ•ˆ

        Args:
            conversations: å¯¹è¯åˆ—è¡¨

        Returns:
            {
                'de_score': float,  # 0-100
                'details': {
                    'avg_info_density': float,      # å¹³å‡ä¿¡æ¯å¯†åº¦
                    'repetition_rate': float,       # é‡å¤ç‡
                    'meaningless_rate': float,      # æ— æ„ä¹‰å‘è¨€ç‡
                    'total_rounds': int             # æ€»è½®æ¬¡
                }
            }
        """
        if not conversations:
            return {'de_score': 0, 'details': {}}

        # 1. ä¿¡æ¯å¯†åº¦ï¼ˆåŸºäºç‹¬ç‰¹è¯æ±‡æ•° / æ€»å­—æ•°ï¼‰
        info_density = self._calculate_info_density(conversations)

        # 2. é‡å¤ç‡
        repetition_rate = self._calculate_repetition_rate(conversations)

        # 3. æ— æ„ä¹‰å‘è¨€ç‡
        meaningless_rate = self._calculate_meaningless_rate(conversations)

        # ç»¼åˆå¾—åˆ†
        de_score = (
            info_density * 0.4 +                    # ä¿¡æ¯å¯†åº¦å 40%
            (1 - repetition_rate) * 0.3 +           # ä½é‡å¤ç‡å 30%
            (1 - meaningless_rate) * 0.3            # ä½æ— æ„ä¹‰ç‡å 30%
        ) * 100

        return {
            'de_score': round(de_score, 2),
            'details': {
                'avg_info_density': round(info_density, 2),
                'repetition_rate': round(repetition_rate, 2),
                'meaningless_rate': round(meaningless_rate, 2),
                'total_rounds': len(conversations)
            }
        }

    def _calculate_info_density(self, conversations: List[Dict]) -> float:
        """è®¡ç®—ä¿¡æ¯å¯†åº¦"""
        all_words = []
        for msg in conversations:
            content = msg.get('content', '')
            words = re.findall(r'\w+', content.lower())
            all_words.extend(words)

        if not all_words:
            return 0

        # ç‹¬ç‰¹è¯æ±‡æ•° / æ€»è¯æ±‡æ•°
        unique_ratio = len(set(all_words)) / len(all_words)
        return unique_ratio

    def _calculate_repetition_rate(self, conversations: List[Dict]) -> float:
        """è®¡ç®—é‡å¤ç‡ï¼ˆç›¸ä¼¼å†…å®¹å æ¯”ï¼‰"""
        contents = [msg.get('content', '') for msg in conversations]

        if len(contents) < 2:
            return 0

        # ç®€å•çš„é‡å¤æ£€æµ‹ï¼šç›¸åŒå†…å®¹æˆ–é«˜åº¦ç›¸ä¼¼å†…å®¹
        repetitions = 0
        for i in range(len(contents)):
            for j in range(i + 1, len(contents)):
                similarity = self._simple_text_similarity(contents[i], contents[j])
                if similarity > 0.7:  # ç›¸ä¼¼åº¦é˜ˆå€¼
                    repetitions += 1

        max_pairs = len(contents) * (len(contents) - 1) / 2
        return repetitions / max_pairs if max_pairs > 0 else 0

    def _calculate_meaningless_rate(self, conversations: List[Dict]) -> float:
        """è®¡ç®—æ— æ„ä¹‰å‘è¨€ç‡"""
        # å®šä¹‰æ— æ„ä¹‰æ¨¡å¼ï¼ˆè¿‡çŸ­ã€è¿‡äºé€šç”¨ç­‰ï¼‰
        meaningless_patterns = [
            r'^(å¥½çš„?|æ˜¯çš„?|å—¯|å“¦|å•Š|å‘ƒ|å˜¿|å–‚)$',
            r'^\.{3,}$',  # åªæœ‰çœç•¥å·
            r'^[ï¼ï¼Ÿã€‚ï¼Œ]{1,3}$',  # åªæœ‰æ ‡ç‚¹
        ]

        meaningless_count = 0
        for msg in conversations:
            content = msg.get('content', '').strip()

            # æ£€æŸ¥æ˜¯å¦è¿‡çŸ­ï¼ˆ<5å­—ç¬¦ï¼‰
            if len(content) < 5:
                meaningless_count += 1
                continue

            # æ£€æŸ¥æ˜¯å¦åŒ¹é…æ— æ„ä¹‰æ¨¡å¼
            for pattern in meaningless_patterns:
                if re.match(pattern, content):
                    meaningless_count += 1
                    break

        return meaningless_count / len(conversations) if conversations else 0

    def _simple_text_similarity(self, text1: str, text2: str) -> float:
        """ç®€å•çš„æ–‡æœ¬ç›¸ä¼¼åº¦è®¡ç®—ï¼ˆåŸºäºè¯æ±‡é‡å ï¼‰"""
        words1 = set(re.findall(r'\w+', text1.lower()))
        words2 = set(re.findall(r'\w+', text2.lower()))

        if not words1 or not words2:
            return 0

        intersection = len(words1 & words2)
        union = len(words1 | words2)

        return intersection / union if union > 0 else 0

    # ==================== 3. OOC ç‡æ£€æµ‹ ====================

    def calculate_ooc_rate(self, conversations: List[Dict],
                          character_profiles: Dict[str, str]) -> Dict:
        """
        è®¡ç®— OOC ç‡ï¼ˆOut of Character Rateï¼‰

        æ£€æµ‹è§’è‰²å‘è¨€æ˜¯å¦ç¬¦åˆäººè®¾

        Args:
            conversations: å¯¹è¯åˆ—è¡¨
            character_profiles: è§’è‰²äººè®¾ {'è§’è‰²å': 'æ€§æ ¼æè¿°', ...}

        Returns:
            {
                'ooc_rate': float,  # 0-100ï¼Œè¶Šä½è¶Šå¥½
                'ooc_cases': list,  # OOC æ¡ˆä¾‹
                'details': dict
            }
        """
        if not self.llm:
            return {
                'ooc_rate': 0,
                'message': 'éœ€è¦ API Key æ‰èƒ½è¿›è¡Œ OOC æ£€æµ‹',
                'ooc_cases': []
            }

        # æŒ‰è§’è‰²åˆ†ç»„
        character_messages = defaultdict(list)
        for msg in conversations:
            speaker = msg.get('speaker')
            if speaker and speaker in character_profiles:
                character_messages[speaker].append(msg)

        ooc_cases = []
        total_checked = 0

        # å¯¹æ¯ä¸ªè§’è‰²çš„å‘è¨€è¿›è¡Œé‡‡æ ·æ£€æµ‹ï¼ˆæœ€å¤šæ£€æŸ¥æ¯è§’è‰²5æ¡ï¼‰
        for char, messages in character_messages.items():
            profile = character_profiles[char]
            sample_size = min(5, len(messages))

            # å‡åŒ€é‡‡æ ·
            step = len(messages) // sample_size if sample_size > 0 else 1
            sampled = messages[::step][:sample_size]

            for msg in sampled:
                total_checked += 1
                is_ooc = self._check_ooc_with_llm(
                    character_name=char,
                    personality=profile,
                    message=msg['content']
                )

                if is_ooc:
                    ooc_cases.append({
                        'character': char,
                        'content': msg['content'],
                        'expected_personality': profile
                    })

        ooc_rate = (len(ooc_cases) / total_checked * 100) if total_checked > 0 else 0

        return {
            'ooc_rate': round(ooc_rate, 2),
            'ooc_cases': ooc_cases,
            'details': {
                'total_checked': total_checked,
                'ooc_count': len(ooc_cases)
            }
        }

    def _check_ooc_with_llm(self, character_name: str,
                           personality: str, message: str) -> bool:
        """ä½¿ç”¨ LLM æ£€æµ‹å•æ¡å‘è¨€æ˜¯å¦ OOC"""
        try:
            prompt = f"""
ä½ æ˜¯ä¸€ä¸ªè§’è‰²æ€§æ ¼ä¸€è‡´æ€§æ£€æµ‹ä¸“å®¶ã€‚è¯·åˆ¤æ–­ä»¥ä¸‹å‘è¨€æ˜¯å¦ç¬¦åˆè§’è‰²äººè®¾ã€‚

è§’è‰²åç§°ï¼š{character_name}
è§’è‰²æ€§æ ¼ï¼š{personality}

å‘è¨€å†…å®¹ï¼š{message}

è¯·å›ç­”ï¼šè¿™æ¡å‘è¨€æ˜¯å¦ç¬¦åˆè¯¥è§’è‰²çš„æ€§æ ¼ï¼Ÿï¼ˆåªéœ€å›ç­”"ç¬¦åˆ"æˆ–"ä¸ç¬¦åˆ"ï¼Œä¸è¦è§£é‡Šï¼‰
"""

            response = self.llm.models.generate_content(
                model='gemini-2.0-flash-exp',
                contents=prompt
            )

            result = response.text.strip()
            return 'ä¸ç¬¦åˆ' in result

        except Exception as e:
            print(f"OOC æ£€æµ‹å¤±è´¥: {e}")
            return False

    # ==================== ç»¼åˆè¯„ä¼° ====================

    def comprehensive_evaluation(self, conversations: List[Dict],
                                character_profiles: Optional[Dict[str, str]] = None) -> Dict:
        """
        ç»¼åˆè¯„ä¼°

        Args:
            conversations: å¯¹è¯åˆ—è¡¨
            character_profiles: è§’è‰²äººè®¾ï¼ˆå¯é€‰ï¼Œç”¨äº OOC æ£€æµ‹ï¼‰

        Returns:
            å®Œæ•´çš„è¯„ä¼°æŠ¥å‘Š
        """
        # 1. äººè®¾ç¦»æ•£åº¦
        cpd_result = self.calculate_cpd(conversations)

        # 2. å¯¹è¯æœ‰æ•ˆç‡
        de_result = self.calculate_de(conversations)

        # 3. OOC ç‡ï¼ˆå¦‚æœæä¾›äº†äººè®¾ï¼‰
        ooc_result = None
        if character_profiles:
            ooc_result = self.calculate_ooc_rate(conversations, character_profiles)

        # ç»¼åˆå¾—åˆ†ï¼ˆåŠ æƒå¹³å‡ï¼‰
        scores = [cpd_result['cpd_score'], de_result['de_score']]
        if ooc_result:
            # OOC ç‡è¶Šä½è¶Šå¥½ï¼Œè½¬æ¢ä¸ºå¾—åˆ†
            ooc_score = 100 - ooc_result['ooc_rate']
            scores.append(ooc_score)

        overall_score = sum(scores) / len(scores)

        return {
            'overall_score': round(overall_score, 2),
            'timestamp': datetime.now().isoformat(),
            'metrics': {
                'cpd': cpd_result,
                'de': de_result,
                'ooc': ooc_result
            },
            'summary': {
                'total_messages': len(conversations),
                'unique_characters': len(set(msg.get('speaker') for msg in conversations if msg.get('speaker')))
            }
        }


# ==================== å·¥å…·å‡½æ•° ====================

def save_evaluation_report(report: Dict, output_path: str):
    """ä¿å­˜è¯„ä¼°æŠ¥å‘Šä¸º JSON"""
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    print(f"âœ… è¯„ä¼°æŠ¥å‘Šå·²ä¿å­˜: {output_path}")


def print_evaluation_report(report: Dict):
    """æ‰“å°è¯„ä¼°æŠ¥å‘Šï¼ˆå‹å¥½æ ¼å¼ï¼‰"""
    print("\n" + "="*60)
    print("ğŸ“Š è¯„ä¼°æŠ¥å‘Š")
    print("="*60)

    print(f"\nğŸ¯ ç»¼åˆå¾—åˆ†: {report['overall_score']}/100")
    print(f"ğŸ“… è¯„ä¼°æ—¶é—´: {report['timestamp']}")

    print(f"\nğŸ“ˆ åŸºç¡€ç»Ÿè®¡:")
    summary = report['summary']
    print(f"  â€¢ æ€»æ¶ˆæ¯æ•°: {summary['total_messages']}")
    print(f"  â€¢ å‚ä¸è§’è‰²æ•°: {summary['unique_characters']}")

    metrics = report['metrics']

    # CPD
    print(f"\nğŸ’ äººè®¾ç¦»æ•£åº¦ (CPD): {metrics['cpd']['cpd_score']}/100")
    cpd_details = metrics['cpd']['details']
    print(f"  â€¢ è¯æ±‡å¤šæ ·æ€§: {cpd_details.get('vocabulary_diversity', 0)}/100")
    print(f"  â€¢ é•¿åº¦å·®å¼‚: {cpd_details.get('length_variance', 0)}/100")
    print(f"  â€¢ æ ‡ç‚¹é£æ ¼: {cpd_details.get('punctuation_diversity', 0)}/100")

    # DE
    print(f"\nâš¡ å¯¹è¯æœ‰æ•ˆç‡ (DE): {metrics['de']['de_score']}/100")
    de_details = metrics['de']['details']
    print(f"  â€¢ ä¿¡æ¯å¯†åº¦: {de_details.get('avg_info_density', 0):.2f}")
    print(f"  â€¢ é‡å¤ç‡: {de_details.get('repetition_rate', 0)*100:.1f}%")
    print(f"  â€¢ æ— æ„ä¹‰ç‡: {de_details.get('meaningless_rate', 0)*100:.1f}%")

    # OOC
    if metrics.get('ooc'):
        ooc = metrics['ooc']
        print(f"\nğŸ­ OOC ç‡: {ooc['ooc_rate']:.1f}%")
        if ooc.get('ooc_cases'):
            print(f"  â€¢ æ£€æµ‹åˆ° {len(ooc['ooc_cases'])} ä¸ª OOC æ¡ˆä¾‹")

    print("\n" + "="*60)


if __name__ == "__main__":
    # æµ‹è¯•ç¤ºä¾‹
    print("è¯„æµ‹ç³»ç»Ÿæµ‹è¯•")

    # ç¤ºä¾‹å¯¹è¯æ•°æ®
    test_conversations = [
        {'speaker': 'å‹‡å£«', 'content': 'è®©æˆ‘æ‰“å¤´é˜µï¼æˆ‘ä¸æ€•å±é™©ï¼'},
        {'speaker': 'æ³•å¸ˆ', 'content': 'ç­‰ç­‰ï¼Œè®©æˆ‘å…ˆåˆ†æä¸€ä¸‹é­”æ³•æ³¢åŠ¨...'},
        {'speaker': 'ç›—è´¼', 'content': 'å˜¿å˜¿ï¼Œæˆ‘å»æ¢æ¢è·¯ï¼Œå°å¿ƒé™·é˜±ã€‚'},
        {'speaker': 'å‹‡å£«', 'content': 'å‰é¢æœ‰æ•Œäººï¼Œå†²å•Šï¼'},
        {'speaker': 'æ³•å¸ˆ', 'content': 'è¿™ä¸ªå’’è¯­éœ€è¦3ç§’æ–½æ³•æ—¶é—´...'},
        {'speaker': 'ç›—è´¼', 'content': 'æˆ‘å·²ç»ç»•åˆ°åé¢äº†ï¼Œå‡†å¤‡å·è¢­ã€‚'}
    ]

    evaluator = EvaluationMetrics()

    # è®¡ç®— CPD
    cpd_result = evaluator.calculate_cpd(test_conversations)
    print(f"\näººè®¾ç¦»æ•£åº¦: {cpd_result['cpd_score']}/100")

    # è®¡ç®— DE
    de_result = evaluator.calculate_de(test_conversations)
    print(f"å¯¹è¯æœ‰æ•ˆç‡: {de_result['de_score']}/100")

    # ç»¼åˆè¯„ä¼°
    character_profiles = {
        'å‹‡å£«': 'å‹‡æ•¢ã€å†²åŠ¨ã€å–œæ¬¢å†’é™©',
        'æ³•å¸ˆ': 'èªæ˜ã€è°¨æ…ã€å–„äºåˆ†æ',
        'ç›—è´¼': 'ç‹¡çŒ¾ã€æœºæ™ºã€è¡ŒåŠ¨è¿…é€Ÿ'
    }

    report = evaluator.comprehensive_evaluation(test_conversations, character_profiles)
    print_evaluation_report(report)
