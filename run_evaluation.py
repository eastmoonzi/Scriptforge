"""
è‡ªåŠ¨åŒ–è¯„æµ‹è„šæœ¬
è¿è¡Œæµ‹è¯•æ•°æ®é›†å¹¶ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š
"""

import json
import os
from datetime import datetime
from typing import List, Dict, Optional
from evaluation_system import EvaluationMetrics, print_evaluation_report, save_evaluation_report


class AutomatedEvaluationRunner:
    """è‡ªåŠ¨åŒ–è¯„æµ‹è¿è¡Œå™¨"""

    def __init__(self, api_key: Optional[str] = None):
        """
        åˆå§‹åŒ–è¯„æµ‹è¿è¡Œå™¨

        Args:
            api_key: Gemini API Keyï¼ˆå¯é€‰ï¼Œç”¨äº OOC æ£€æµ‹ï¼‰
        """
        self.evaluator = EvaluationMetrics(api_key=api_key)
        self.results = []

    def load_test_dataset(self, dataset_path: str = "test_dataset.json") -> Dict:
        """åŠ è½½æµ‹è¯•æ•°æ®é›†"""
        if not os.path.exists(dataset_path):
            raise FileNotFoundError(f"æµ‹è¯•æ•°æ®é›†ä¸å­˜åœ¨: {dataset_path}")

        with open(dataset_path, 'r', encoding='utf-8') as f:
            return json.load(f)

    def run_single_scenario(self, scenario: Dict) -> Dict:
        """
        è¯„ä¼°å•ä¸ªåœºæ™¯

        Args:
            scenario: åœºæ™¯æ•°æ®

        Returns:
            è¯„ä¼°ç»“æœ
        """
        print(f"\n{'='*60}")
        print(f"ğŸ“‹ è¯„ä¼°åœºæ™¯: {scenario['name']} (ID: {scenario['id']})")
        print(f"{'='*60}")

        conversations = scenario.get('conversations', [])
        characters = scenario.get('characters', {})

        # è¿è¡Œç»¼åˆè¯„ä¼°
        report = self.evaluator.comprehensive_evaluation(
            conversations=conversations,
            character_profiles=characters
        )

        # æ·»åŠ åœºæ™¯å…ƒä¿¡æ¯
        report['scenario_info'] = {
            'id': scenario['id'],
            'name': scenario['name'],
            'quality': scenario.get('quality', 'unknown'),
            'scene': scenario.get('scene', ''),
            'issue': scenario.get('issue', None)
        }

        # å¯¹æ¯”é¢„æœŸæŒ‡æ ‡
        expected = scenario.get('expected_metrics', {})
        if expected:
            report['comparison'] = self._compare_with_expected(report, expected)

        return report

    def _compare_with_expected(self, report: Dict, expected: Dict) -> Dict:
        """å¯¹æ¯”å®é™…ç»“æœä¸é¢„æœŸæŒ‡æ ‡"""
        comparison = {}

        # CPD å¯¹æ¯”
        cpd_actual = report['metrics']['cpd']['cpd_score']
        cpd_expected = expected.get('cpd_score', '')

        comparison['cpd'] = {
            'actual': cpd_actual,
            'expected': cpd_expected,
            'pass': self._check_condition(cpd_actual, cpd_expected)
        }

        # DE å¯¹æ¯”
        de_actual = report['metrics']['de']['de_score']
        de_expected = expected.get('de_score', '')

        comparison['de'] = {
            'actual': de_actual,
            'expected': de_expected,
            'pass': self._check_condition(de_actual, de_expected)
        }

        # OOC å¯¹æ¯”
        if report['metrics'].get('ooc'):
            ooc_actual = report['metrics']['ooc']['ooc_rate']
            ooc_expected = expected.get('ooc_rate', '')

            comparison['ooc'] = {
                'actual': ooc_actual,
                'expected': ooc_expected,
                'pass': self._check_condition(ooc_actual, ooc_expected)
            }

        return comparison

    def _check_condition(self, actual: float, expected: str) -> bool:
        """
        æ£€æŸ¥å®é™…å€¼æ˜¯å¦ç¬¦åˆé¢„æœŸæ¡ä»¶

        Args:
            actual: å®é™…å€¼
            expected: é¢„æœŸæ¡ä»¶ï¼ˆå¦‚ ">70", "<10", "60-75"ï¼‰

        Returns:
            æ˜¯å¦ç¬¦åˆ
        """
        if not expected or expected == "ä»»æ„":
            return True

        # å¤§äºæ¡ä»¶
        if expected.startswith('>'):
            threshold = float(expected[1:])
            return actual > threshold

        # å°äºæ¡ä»¶
        if expected.startswith('<'):
            threshold = float(expected[1:])
            return actual < threshold

        # èŒƒå›´æ¡ä»¶
        if '-' in expected:
            min_val, max_val = map(float, expected.split('-'))
            return min_val <= actual <= max_val

        return True

    def run_all_scenarios(self, dataset_path: str = "test_dataset.json") -> List[Dict]:
        """è¿è¡Œæ‰€æœ‰åœºæ™¯è¯„æµ‹"""
        print("\n" + "="*60)
        print("ğŸš€ å¼€å§‹è‡ªåŠ¨åŒ–è¯„æµ‹")
        print("="*60)

        # åŠ è½½æ•°æ®é›†
        dataset = self.load_test_dataset(dataset_path)
        print(f"\nğŸ“¦ åŠ è½½æ•°æ®é›†: {dataset['dataset_name']}")
        print(f"ğŸ“Š ç‰ˆæœ¬: {dataset['version']}")

        all_scenarios = dataset.get('scenarios', []) + dataset.get('additional_test_cases', [])
        print(f"ğŸ“‹ æµ‹è¯•åœºæ™¯æ•°: {len(all_scenarios)}")

        # è¿è¡Œè¯„æµ‹
        results = []
        for i, scenario in enumerate(all_scenarios, 1):
            print(f"\n[{i}/{len(all_scenarios)}] è¯„ä¼°ä¸­...")

            try:
                result = self.run_single_scenario(scenario)
                results.append(result)

                # æ‰“å°ç®€è¦ç»“æœ
                self._print_brief_result(result)

            except Exception as e:
                print(f"âŒ è¯„ä¼°å¤±è´¥: {str(e)}")
                results.append({
                    'scenario_info': {
                        'id': scenario.get('id'),
                        'name': scenario.get('name'),
                        'error': str(e)
                    }
                })

        self.results = results
        return results

    def _print_brief_result(self, result: Dict):
        """æ‰“å°ç®€è¦ç»“æœ"""
        scenario_info = result.get('scenario_info', {})
        quality = scenario_info.get('quality', 'unknown')

        print(f"\nâœ… è¯„ä¼°å®Œæˆ")
        print(f"   è´¨é‡æ ‡ç­¾: {quality}")
        print(f"   ç»¼åˆå¾—åˆ†: {result.get('overall_score', 0):.1f}/100")

        # æ‰“å°å¯¹æ¯”ç»“æœ
        if result.get('comparison'):
            comp = result['comparison']
            print(f"\n   ğŸ“Š æŒ‡æ ‡å¯¹æ¯”:")

            for metric, data in comp.items():
                status = "âœ…" if data['pass'] else "âŒ"
                print(f"      {metric.upper()}: {data['actual']:.1f} (é¢„æœŸ: {data['expected']}) {status}")

    def generate_summary_report(self) -> Dict:
        """ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š"""
        if not self.results:
            return {}

        # æŒ‰è´¨é‡åˆ†ç»„ç»Ÿè®¡
        quality_groups = {}
        for result in self.results:
            quality = result.get('scenario_info', {}).get('quality', 'unknown')

            if quality not in quality_groups:
                quality_groups[quality] = []

            quality_groups[quality].append(result)

        # è®¡ç®—ç»Ÿè®¡æ•°æ®
        summary = {
            'total_scenarios': len(self.results),
            'timestamp': datetime.now().isoformat(),
            'by_quality': {}
        }

        for quality, results in quality_groups.items():
            scores = [r.get('overall_score', 0) for r in results]

            summary['by_quality'][quality] = {
                'count': len(results),
                'avg_score': sum(scores) / len(scores) if scores else 0,
                'min_score': min(scores) if scores else 0,
                'max_score': max(scores) if scores else 0
            }

        # é€šè¿‡ç‡ç»Ÿè®¡ï¼ˆå¦‚æœæœ‰å¯¹æ¯”æ•°æ®ï¼‰
        pass_count = 0
        total_with_comparison = 0

        for result in self.results:
            if result.get('comparison'):
                total_with_comparison += 1
                # æ£€æŸ¥æ‰€æœ‰æŒ‡æ ‡æ˜¯å¦éƒ½é€šè¿‡
                all_pass = all(data['pass'] for data in result['comparison'].values())
                if all_pass:
                    pass_count += 1

        if total_with_comparison > 0:
            summary['pass_rate'] = pass_count / total_with_comparison * 100
            summary['pass_count'] = pass_count
            summary['total_with_comparison'] = total_with_comparison

        return summary

    def print_summary_report(self):
        """æ‰“å°æ±‡æ€»æŠ¥å‘Š"""
        summary = self.generate_summary_report()

        print("\n" + "="*60)
        print("ğŸ“Š è¯„æµ‹æ±‡æ€»æŠ¥å‘Š")
        print("="*60)

        print(f"\næ€»åœºæ™¯æ•°: {summary['total_scenarios']}")
        print(f"è¯„æµ‹æ—¶é—´: {summary['timestamp']}")

        if summary.get('pass_rate') is not None:
            print(f"\nâœ… é€šè¿‡ç‡: {summary['pass_rate']:.1f}%")
            print(f"   é€šè¿‡: {summary['pass_count']}/{summary['total_with_comparison']}")

        print("\nğŸ“ˆ æŒ‰è´¨é‡åˆ†ç»„ç»Ÿè®¡:")
        for quality, stats in summary['by_quality'].items():
            print(f"\n   ã€{quality.upper()}ã€‘")
            print(f"      æ•°é‡: {stats['count']}")
            print(f"      å¹³å‡å¾—åˆ†: {stats['avg_score']:.1f}/100")
            print(f"      å¾—åˆ†èŒƒå›´: {stats['min_score']:.1f} - {stats['max_score']:.1f}")

        # æå‡å»ºè®®
        print("\nğŸ’¡ æ”¹è¿›å»ºè®®:")
        if 'low' in summary['by_quality']:
            low_avg = summary['by_quality']['low']['avg_score']
            if low_avg > 50:
                print("   âš ï¸ ä½è´¨é‡æ ·æœ¬çš„å¾—åˆ†åé«˜ï¼Œè¯„æµ‹ç³»ç»Ÿå¯èƒ½éœ€è¦è°ƒæ•´æƒé‡")

        if 'high' in summary['by_quality']:
            high_avg = summary['by_quality']['high']['avg_score']
            if high_avg < 70:
                print("   âš ï¸ é«˜è´¨é‡æ ·æœ¬çš„å¾—åˆ†åä½ï¼Œç³»ç»Ÿä»æœ‰æ”¹è¿›ç©ºé—´")

        print("\n" + "="*60)

    def save_full_report(self, output_dir: str = "evaluation_reports"):
        """ä¿å­˜å®Œæ•´è¯„æµ‹æŠ¥å‘Š"""
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        # ä¿å­˜è¯¦ç»†ç»“æœ
        detail_path = os.path.join(output_dir, f"detailed_results_{timestamp}.json")
        with open(detail_path, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, ensure_ascii=False, indent=2)
        print(f"\nğŸ’¾ è¯¦ç»†ç»“æœå·²ä¿å­˜: {detail_path}")

        # ä¿å­˜æ±‡æ€»æŠ¥å‘Š
        summary = self.generate_summary_report()
        summary_path = os.path.join(output_dir, f"summary_{timestamp}.json")
        with open(summary_path, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        print(f"ğŸ’¾ æ±‡æ€»æŠ¥å‘Šå·²ä¿å­˜: {summary_path}")

        return detail_path, summary_path


def main():
    """ä¸»å‡½æ•° - å‘½ä»¤è¡Œå…¥å£"""
    import argparse

    parser = argparse.ArgumentParser(description="GroupChat è¯„æµ‹ç³»ç»Ÿ")
    parser.add_argument('--dataset', default='test_dataset.json', help='æµ‹è¯•æ•°æ®é›†è·¯å¾„')
    parser.add_argument('--api-key', default=None, help='Gemini API Keyï¼ˆç”¨äºOOCæ£€æµ‹ï¼‰')
    parser.add_argument('--output-dir', default='evaluation_reports', help='æŠ¥å‘Šè¾“å‡ºç›®å½•')
    parser.add_argument('--no-save', action='store_true', help='ä¸ä¿å­˜æŠ¥å‘Šæ–‡ä»¶')

    args = parser.parse_args()

    # å¦‚æœæ²¡æœ‰æä¾› API Keyï¼Œå°è¯•ä»ç¯å¢ƒå˜é‡è·å–
    api_key = args.api_key or os.getenv('GEMINI_API_KEY')

    if not api_key:
        print("âš ï¸  æœªæä¾› API Keyï¼Œå°†è·³è¿‡ OOC æ£€æµ‹")
        print("   æç¤º: å¯é€šè¿‡ --api-key å‚æ•°æˆ– GEMINI_API_KEY ç¯å¢ƒå˜é‡æä¾›")

    # åˆ›å»ºè¯„æµ‹è¿è¡Œå™¨
    runner = AutomatedEvaluationRunner(api_key=api_key)

    # è¿è¡Œè¯„æµ‹
    try:
        results = runner.run_all_scenarios(dataset_path=args.dataset)

        # æ‰“å°æ±‡æ€»
        runner.print_summary_report()

        # ä¿å­˜æŠ¥å‘Š
        if not args.no_save:
            runner.save_full_report(output_dir=args.output_dir)

        print("\nâœ… è¯„æµ‹å®Œæˆï¼")

    except Exception as e:
        print(f"\nâŒ è¯„æµ‹å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
